import torch

# è·¯å¾„
embedding_path = 'Data/embedding/semantic_embedding.pt'

# åŠ è½½æ–‡ä»¶
data = torch.load(embedding_path, map_location='cpu')

# è§£åŒ…
embedding_matrix = data['embedding_matrix']
id2index = data['id2index']
index2id = data['index2id']

# æ ¡éªŒç»“æ„
assert isinstance(id2index, dict), "id2index åº”ä¸º dict[str â†’ int]"
assert isinstance(index2id, dict), "index2id åº”ä¸º dict[int â†’ str]"
assert isinstance(embedding_matrix, torch.Tensor), "embedding_matrix åº”ä¸º torch.Tensor"

# ç»´åº¦æ£€æŸ¥
n_vecs, dim = embedding_matrix.shape
print(f"âœ… å‘é‡çŸ©é˜µç»´åº¦: {embedding_matrix.shape}")
print(f"âœ… id2index é•¿åº¦: {len(id2index)}")
print(f"âœ… index2id é•¿åº¦: {len(index2id)}")

assert n_vecs == len(id2index) == len(index2id), "æ•°é‡ä¸ä¸€è‡´ï¼"

# æ ·ä¾‹éªŒè¯å‰5ä¸ª
print("\nğŸ” ç¤ºä¾‹:")
for i in range(5):
    eid = index2id[i]
    idx = id2index[eid]
    vec = embedding_matrix[idx]
    print(f"{i}. {eid} â†’ index: {idx} â†’ vec[:5]: {vec[:5].tolist()}")